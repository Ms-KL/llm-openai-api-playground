{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM App with Langchain\n",
    "\n",
    "* [Datacamp Tutorial](https://www.datacamp.com/tutorial/how-to-build-llm-applications-with-langchain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Large Language Models (LLMs)?\n",
    "\n",
    "* AI systems designed to understand and generate human-like text.\n",
    "* LLM Models are trained on large amounts of text data, enabling them to:\n",
    "    * Understand the context of a given text.\n",
    "    * Generate text that is coherent and relevant to the context.\n",
    "    * Perform language-related tasks such as: \n",
    "        * translation\n",
    "        * summarization\n",
    "        * question answering\n",
    "        * text completion\n",
    "\n",
    "## What is Langchain?\n",
    "\n",
    "* Langchain is a Python library that makes it easy to use LLMs.\n",
    "* It comes with a collection of APIs that allow you to:\n",
    "    * generate text\n",
    "    * perform language-related tasks\n",
    "    * fine-tune LLMs on your own data\n",
    "* Langchain is built on top of the [Hugging Face Transformers library](https://huggingface.co/transformers/).\n",
    "* Langchain can be used to:\n",
    "    * tailor prompts\n",
    "    * construct chain link models\n",
    "    * fine-tune LLMs on your own data\n",
    "    * integrate models for use in production (eg: GPT, Hugging Face, etc.)\n",
    "    * manipulate context for precision and recall\n",
    "\n",
    "Read More: [Introduction to LangChain for Data Engineering & Data Applications](https://www.datacamp.com/tutorial/introduction-to-lanchain-for-data-engineering-and-data-applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components of LangChain\n",
    "\n",
    "### **Components and Chains**\n",
    "* **Components** = building blocks of Langchain.\n",
    "    * these modules perform specific functions in the language processing pipeline\n",
    "    * they can be combined to create **chains** for tailored workflows and apps:\n",
    "        * eg: sentiment analysis, intent recognition, text generation, etc.\n",
    "* **Chains** = collection of components that are used to perform a specific task.\n",
    "* **Chain Links** = individual components that make up a chain.\n",
    "\n",
    "### **Prompt Templates**\n",
    "* a collection of prompts that are used to generate text.\n",
    "    * they are reusable predefined prompts across chains\n",
    "    * these templates can become dynamic and adaptable by inserting specific \"values\"\n",
    "        * eg: a prompt asking for a user's name could be personalized by inserting a specific value\n",
    "    * this feature is beneficial for generating prompts based on dynamic resources\n",
    "* are used to:\n",
    "    * generate text\n",
    "    * perform language-related tasks\n",
    "    * fine-tune LLMs on your own data\n",
    "    * tailor prompts\n",
    "    * construct chain link models\n",
    "    * fine-tune LLMs on your own data\n",
    "    * integrate models for use in production (eg: GPT, Hugging Face, etc.)\n",
    "    * manipulate context for precision and recall\n",
    "\n",
    "### **Vector Stores**\n",
    "* essentially analyze numerical representations of document meanings\n",
    "* serves as a storage facility for these embeddings, allowing efficient search based on semantic similarity.\n",
    "* used to:\n",
    "    * store and search information via embeddings\n",
    "    * analyze numerical representations of document meanings\n",
    "    * serve as a storage facility for these embeddings\n",
    "    * allow efficient search based on semantic similarity\n",
    "\n",
    "### **Indexes and retrievers**\n",
    "* **Indexes** act as databases storing details and metadata about the model's training data\n",
    "* **Retrievers** swiftly search this index for specific information\n",
    "* **Indexes and retrievers** are used to:\n",
    "    * store details and metadata about the model's training data\n",
    "    * swiftly search this index for specific information\n",
    "    * improve the model's responses by providing context and related information\n",
    "\n",
    "### **Output Parsers**\n",
    "* used to:\n",
    "    * manage and refine the responses generated by the model\n",
    "    * eliminate undesired content\n",
    "    * tailor the output format\n",
    "    * supplement extra data to the response\n",
    "    * extract structured results, like JSON objects, from the language model's responses\n",
    "\n",
    "### **Example selectors**\n",
    "* used to:\n",
    "    * identify appropriate instances from the model's training data\n",
    "    * improve the precision and pertinence of the generated responses\n",
    "    * adjust to favor certain types of examples or filter out unrelated ones\n",
    "    * provide a tailored AI response based on user input\n",
    "\n",
    "### **Agents**\n",
    "* are unique LangChain instances, each with specific prompts, memory, and chain for a particular use case.\n",
    "* can be deployed on various platforms, including web, mobile, and chatbots, catering to a wide audience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Langchain in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Activate the environment: \n",
    "    - Conda: `conda activate envpy39`\n",
    "    - venv: `source venv/bin/activate`\n",
    "    - VS Code: `Ctrl+Shift+P` > `Python: Select Interpreter` > `Python 3.9.7 64-bit ('envpy39': conda)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain -q\n",
    "# or\n",
    "# install langchain -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Setup the key as an environment variable:\n",
    "    - Create a file called: `.env` \n",
    "    - Get OpenAI Token\n",
    "    - Set the variable in the .env file: `OPENAI_API_KEY=\"...\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Setup the key in the relevant class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an LLM Powered App with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "What data scientist did the data scientist?\n",
      "\n",
      "The data scientist did the data scientist?\n",
      "\n",
      "Because the data scientist knew what he was doing, he was able to churn out data that featsed almost every analysis imaginable.\n"
     ]
    }
   ],
   "source": [
    "# Using OpenAI API\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=API_KEY)\n",
    "\n",
    "print(llm(\"Tell me a joke about data scientist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using HuggingFace API\n",
    "# retrieve a token and set the token value in .env file\n",
    "\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(repo_id = \"google/flan-t5-xl\", huggingfacehub_api_token = API_KEY)\n",
    "\n",
    "print(llm(\"Tell me a joke about data scientist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation(text='\\n\\nWhat data scientist did the data scientist?\\n\\nThe data scientist did the data scientist?\\n\\nBecause the data scientist knew how to use data to make sense of it all, they were also able to make sense of it all and come up with something better.', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
     ]
    }
   ],
   "source": [
    "# Experiment with multiple prompts\n",
    "\n",
    "llm_response = llm.generate(['Tell me a joke about data scientist',\n",
    "\n",
    "'Tell me a joke about recruiter',\n",
    "\n",
    "'Tell me a joke about psychologist'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\nWhat data scientist did the data scientist?\\n\\nThe data scientist did the data scientist?\\n\\nBecause the data scientist knew how to use data to make sense of it all, they were also able to make sense of it all and come up with something better.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the recruiter find a busy person?\\n\\nBecause they were too busy to ask for help.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the psychologist say that he never needed a new approach to his work?\\n\\nBecause he had already found several old ones that worked well.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 20, 'total_tokens': 131}, 'model_name': 'text-ada-001'}\n"
     ]
    }
   ],
   "source": [
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='\\n\\nWhat data scientist did the data scientist?\\n\\nThe data scientist did the data scientist?\\n\\nBecause the data scientist knew how to use data to make sense of it all, they were also able to make sense of it all and come up with something better.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the recruiter find a busy person?\\n\\nBecause they were too busy to ask for help.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nWhy did the psychologist say that he never needed a new approach to his work?\\n\\nBecause he had already found several old ones that worked well.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
     ]
    }
   ],
   "source": [
    "print(llm_response.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation(text='\\n\\nWhat data scientist did the data scientist?\\n\\nThe data scientist did the data scientist?\\n\\nBecause the data scientist knew how to use data to make sense of it all, they were also able to make sense of it all and come up with something better.', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
     ]
    }
   ],
   "source": [
    "print(llm_response.generations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Prompt Templates for LLMs in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output: \n",
      "\n",
      "1. Visit the Louvre Museum and see the Mona Lisa and other famous artworks\n",
      "2. Climb the Eiffel Tower and take in the incredible views of Paris\n",
      "3. Enjoy a leisurely stroll along the Champs-Élysées and admire the iconic architecture\n"
     ]
    }
   ],
   "source": [
    "USER_INPUT = 'Paris'\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=API_KEY)\n",
    "\n",
    "template = \"\"\" I am travelling to {location}. What are the top 3 things I can do while I am there. Be very specific and respond as three bullet points \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "\n",
    "input_variables=[\"location\"],\n",
    "\n",
    "template=template,\n",
    "\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=USER_INPUT )\n",
    "\n",
    "print(f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output: \n",
      "\n",
      "1. Visit the Great Barrier Reef; take a boat tour and snorkel or scuba dive to explore the incredible marine life. \n",
      "\n",
      "2. Go on a road trip along the Great Ocean Road; witness the stunning coastline, rainforest, and wildlife. \n",
      "\n",
      "3. Take a walk to the top of Sydney Harbour Bridge; enjoy the incredible views of the harbour and the city skyline.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prompt for user input\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=API_KEY)\n",
    "\n",
    "template = \"\"\" I am travelling to {location}. What are the top 3 things I can do while I am there. Be very specific and respond as three bullet points \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "\n",
    "input_variables=[\"location\"],\n",
    "\n",
    "template=template,\n",
    "\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location=input())\n",
    "\n",
    "print(f\"LLM Output: {llm(final_prompt)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining LLMs and Prompts in Multi-Step Workflows\n",
    "\n",
    "* **Multi-step workflows** = a series of steps that are executed in a specific order to achieve a particular goal.\n",
    "    * examples include:\n",
    "        * Sequentially combining multiple LLMs by using the output of the first LLM as input for the second LLM (refer to this section)\n",
    "        * Integrating LLMs with prompt templates\n",
    "        * Merging LLMs with external data, such as for question answering\n",
    "        * Incorporating LLMs with long-term memory, like chat history\n",
    "\n",
    "* EXAMPLE - we create a chain with two components:\n",
    "    * The first component is responsible for identifying the most popular city corresponding to a particular country as input by the user. \n",
    "    * In contrast, the second component focuses on providing information about the top three activities or attractions available for tourists visiting that specific city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Toronto\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m \n",
      "\n",
      "• Explore the CN Tower \n",
      "• Take a stroll through the Distillery District \n",
      "• Check out the Royal Ontario Museum\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=\"text-davinci-003\", \n",
    "    openai_api_key=API_KEY\n",
    "    )\n",
    "\n",
    "# first step in chain\n",
    "\n",
    "template = \"What is the most popular city in {country} for tourists? Just return the name of the city\"\n",
    "\n",
    "first_prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=template\n",
    "    )\n",
    "\n",
    "chain_one = LLMChain(\n",
    "    llm = llm, \n",
    "    prompt = first_prompt\n",
    "    )\n",
    "\n",
    "# second step in chain\n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"city\"],\n",
    "    template=\"What are the top three things to do in this: {city} for tourists. Just return the answer as three bullet points.\",\n",
    "    )\n",
    "\n",
    "chain_two = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=second_prompt\n",
    "    )\n",
    "\n",
    "# Combine the first and the second chain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[\n",
    "        chain_one, \n",
    "        chain_two\n",
    "        ], \n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "final_answer = overall_chain.run(\"Canada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
